# ğŸª© Gemma 3 270M - Universal Browser AI ğŸª©

![Gemma 3 270M Architecture](https://media.datacamp.com/cms/ad_4nxek_-toj_myxu_5eeswj3gudwo5cnych58gtapqmbpelummo-aqiflmyq25ds_cb6uqzutywrseyeo52xx1twwgyzjqosushqzm98awhj19ukum21owg0uammanu3kzvae_bv6ohg.png)

A simple browser-based chat application using **Gemma 270M** with **automatic WebGPU/WASM fallback**, **Transformers.js**, and **Web Workers** for a fully local AI chat experience that works in any modern browser.

## ğŸ§  About Gemma 3 270M

Gemma 3 270M is a compact and efficient AI model from Google's Gemma 3 family, featuring 270 million parameters. It is engineered for fast, task-specific fine-tuning without the need for powerful hardware, making it ideal for on-device applications.

The model's architecture is noteworthy: 170 million parameters are allocated to a vast 256,000-token vocabulary, while the remaining 100 million parameters power its transformer blocks. This structure provides a robust foundation for fine-tuning across various languages and specialized fields. Gemma 3 270M is available in both pre-trained and instruction-tuned versions, excelling at targeted tasks such as text classification, entity extraction, and structured text generation, rather than general, open-ended conversations. It's a text-only model and does not support image input.

### Model Architecture Overview

```
            +---------------------------+
            |      Gemma 3 270M         |
            +---------------------------+
                   |
                   V
+----------------------------------------------------+
|               Compact & Efficient AI Model         |
+----------------------------------------------------+
|                                                    |
|   +-----------------+   +------------------------+ |
|   |  Architecture   |   |      Key Features      | |
|   +-----------------+   +------------------------+ |
|   |                 |   |                        | |
|   | 270M Parameters |-->|--> Small & Lightweight  | |
|   |   - 170M Vocab  |   |--> Specialized Tasks   | |
|   |   - 100M Core   |   |--> Power Efficient      | |
|   |                 |   |--> Large Vocabulary    | |
|   +-----------------+   +------------------------+ |
|                                                    |
+----------------------------------------------------+
                   |
                   V
+----------------------------------------------------+
|                  Ideal For                         |
+----------------------------------------------------+
|                                                    |
|  - On-Device AI (Mobile Phones)                    |
|  - Text Classification                             |
|  - Data Extraction                                 |
|  - Fine-tuning for specific domains                |
|                                                    |
+----------------------------------------------------+
```

**Sources:**
- [Google Developers Blog - Introducing Gemma 3 270M](https://developers.googleblog.com/en/introducing-gemma-3-270m/)
- [Gemma 3 - Google DeepMind](https://deepmind.google/models/gemma/gemma-3/)
- [HuggingFace - google/gemma-3-270m](https://huggingface.co/google/gemma-3-270m)
- [DataCamp - Gemma 3 270M Guide](https://www.datacamp.com/tutorial/gemma-3-270m)

## âœ¨ Features

- ğŸ¤– **Gemma 3 270M ONNX** model running entirely in the browser
- âš¡ **Automatic WebGPU/WASM fallback** - works in any modern browser
- ğŸŒ **Network accessibility** - auto-detects local IP for cross-device access
- ğŸ”„ **Web Workers** for non-blocking model inference
- ğŸ’¬ **Real-time chat interface** with streaming responses
- ğŸ¯ **Context awareness** - maintains conversation history
- ğŸ“± **Responsive design** with modern UI
- ğŸ”’ **Complete privacy** - no data leaves your device

## ğŸ—ï¸ Complete System Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                         GEMMA CHAT APP - FULL ARCHITECTURE              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

                              [User Browser]
                                    â”‚
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚                               â”‚
              â”Œâ”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”                 â”Œâ”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”
              â”‚  index.htmlâ”‚                 â”‚  Vite Dev   â”‚
              â”‚    (UI)    â”‚â—„â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”‚   Server    â”‚
              â”‚            â”‚  WebSocket/HTTP â”‚  Port 3001  â”‚
              â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜                 â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
                    â”‚                               â”‚
         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”           â”Œâ”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”
         â”‚                     â”‚           â”‚   Shutdown    â”‚
    â”Œâ”€â”€â”€â”€â–¼â”€â”€â”€â”€â”         â”Œâ”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”    â”‚   Endpoint    â”‚
    â”‚main.ts  â”‚         â”‚chatManager â”‚    â”‚  /api/shutdownâ”‚
    â”‚WebGPU   â”‚â—„â”€â”€â”€â”€â”€â”€â”€â”€â”¤   .ts      â”‚    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â”‚Detectionâ”‚         â”‚UI Control  â”‚
    â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜         â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜
         â”‚                     â”‚
         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                    â”‚
         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
         â”‚    Web Worker       â”‚
         â”‚  (modelWorker.ts)   â”‚
         â”‚                     â”‚
         â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
         â”‚ â”‚ Transformers.js â”‚ â”‚
         â”‚ â”‚   Pipeline      â”‚ â”‚
         â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
         â”‚          â”‚          â”‚
         â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
         â”‚ â”‚  Gemma 270M     â”‚ â”‚
         â”‚ â”‚  ONNX Model     â”‚ â”‚
         â”‚ â”‚   (~270MB)      â”‚ â”‚
         â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                    â”‚
         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
         â”‚   WebGPU/WASM API   â”‚
         â”‚  Auto-Detection     â”‚
         â”‚                     â”‚
         â”‚ â€¢ WebGPU (if avail) â”‚
         â”‚ â€¢ WASM (fallback)   â”‚
         â”‚ â€¢ Cross-browser     â”‚
         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

DATA FLOW:
=========
1. User Input â†’ Main Thread â†’ Web Worker Message
2. Web Worker â†’ Device Detection â†’ WebGPU/WASM Selection
3. Transformers.js â†’ ONNX Model Load â†’ Compute
4. Streaming Tokens â†’ Web Worker â†’ Main Thread
5. Main Thread â†’ UI Update â†’ User Display

NETWORK ACCESS:
==============
â€¢ Local: http://localhost:3001
â€¢ Network: http://[auto-detected-IP]:3001
â€¢ Cross-device: Access from phones, tablets, other computers

SHUTDOWN FLOW:
=============
1. User clicks Shutdown Button â†’ POST /api/shutdown
2. Express Server â†’ Graceful shutdown â†’ Process exit
3. Alternative: Ctrl+C â†’ SIGINT handler â†’ Clean exit

PERFORMANCE:
===========
â€¢ Model Loading: 30-120s (first), 5-15s (cached)
â€¢ Token Generation: 10-50 tokens/sec
â€¢ Memory Usage: 400-800MB active
â€¢ GPU Utilization: Variable by hardware
```

### Core Components:

1. **Main Thread** (`src/main.ts`, `src/chatManager.ts`)
   - UI management and user interactions
   - Chat message display and input handling
   - Progress tracking and error handling

2. **Model Worker** (`src/modelWorker.ts`)
   - Gemma 270M model initialization
   - Text generation with streaming tokens
   - Conversation context management

3. **Device Detection & Compute**
   - Automatic WebGPU/WASM detection and selection
   - GPU-accelerated inference when available
   - WASM fallback for universal browser support
   - Memory-efficient processing

## ğŸš€ Quick Start

### âš¡ One-liner GitHub Execution (Recommended)

Run the app directly from GitHub without cloning:

```bash
# Run immediately from GitHub (works in any modern browser)
npx github:kaiser/gemma-chat-app
```

This will:
- âœ… Download the app temporarily  
- âœ… Install all dependencies
- âœ… Start the server at http://localhost:3001 + network IP
- âœ… Auto-detect WebGPU/WASM capabilities
- âœ… Work across devices on your local network

### Prerequisites

- Node.js 18+ 
- **Any modern browser** (Chrome, Firefox, Safari, Edge)
- ~500MB available RAM for model
- Optional: WebGPU-enabled browser for faster performance

### Manual Installation (Alternative)

```bash
# Clone and install
git clone <this-repo>
cd gemma-chat-app
npm install

# Start development server
npm run dev
```

Visit: 
- http://localhost:3001 (local access)
- http://[network-ip]:3001 (cross-device access)

### Testing

```bash
# Run Playwright tests with WebGPU
npm run test

# Run with headed browser (for debugging)
npm run test:headed
```

## ğŸ¯ Browser Compatibility

### Automatic Device Detection:
The app automatically detects your browser's capabilities:
- **WebGPU Available**: Uses GPU acceleration for faster inference
- **WebGPU Not Available**: Falls back to WASM for universal compatibility

### Browser Support:
- âœ… Chrome (any version) - WebGPU or WASM
- âœ… Firefox (any version) - WASM fallback
- âœ… Safari (any version) - WASM fallback  
- âœ… Edge (any version) - WebGPU or WASM

### Optional WebGPU Acceleration:
For fastest performance in Chrome/Edge, enable WebGPU flags:
```bash
--enable-unsafe-webgpu
--enable-features=WebGPU
--enable-webgpu-developer-features
```

## ğŸ“Š Performance

### Model Loading:
- **First load**: ~30-120 seconds (downloads 270MB model)
- **Subsequent loads**: ~5-15 seconds (cached)

### Response Generation:
- **Typical response**: 2-10 seconds
- **Streaming**: Tokens appear in real-time
- **Memory usage**: ~400-800MB

### Benchmarks:
```
Model: Gemma 3 270M ONNX
WebGPU Mode: 10-50 tokens/second (depends on GPU)
WASM Mode: 5-25 tokens/second (CPU-based)
Cross-browser: Universal compatibility
```

## ğŸ§ª Testing with Playwright

The project includes comprehensive Playwright tests for:

- âœ… WebGPU availability detection
- âœ… Model loading and initialization
- âœ… Chat interaction and streaming
- âœ… Conversation context maintenance
- âœ… Error handling and recovery
- âœ… Performance monitoring

### Test Configuration:

```typescript
// playwright.config.ts
projects: [
  {
    name: 'chrome-webgpu',
    use: { 
      launchOptions: {
        executablePath: '/usr/bin/google-chrome',
        args: [
          '--enable-unsafe-webgpu',
          '--enable-features=WebGPU',
          // ... more WebGPU flags
        ],
      },
    },
  },
]
```

## ğŸ“ Project Structure

```
gemma-chat-app/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ main.ts           # Application entry point
â”‚   â”œâ”€â”€ chatManager.ts    # Chat logic coordination
â”‚   â””â”€â”€ modelWorker.ts    # Web worker for Gemma inference
â”œâ”€â”€ tests/
â”‚   â””â”€â”€ gemma-chat.spec.ts # Comprehensive test suite
â”œâ”€â”€ index.html            # Main HTML with embedded CSS
â”œâ”€â”€ vite.config.ts        # Vite configuration
â”œâ”€â”€ playwright.config.ts  # Testing configuration
â””â”€â”€ package.json          # Dependencies and scripts
```

## ğŸ”§ Development

### Available Scripts:
```bash
npm run dev         # Start Vite development server
npm run start       # Start server with shutdown endpoint
npm run dev:server  # Alternative server start command
npm run build       # Build for production
npm run preview     # Preview production build
npm run test        # Run Playwright tests

# Convenience scripts
./start-server.sh   # Start server with port check and cleanup
./stop-server.sh    # Stop all server processes on port 3001
```

### Environment Variables:
```bash
# Optional: Custom model endpoint
VITE_MODEL_URL=custom-gemma-model-url
```

## ğŸ¨ Customization

### Modify Model Parameters:
```typescript
// src/modelWorker.ts
const output = await generator(messages, {
  max_new_tokens: 256,    // Response length
  temperature: 0.7,       // Creativity (0-1)
  top_p: 0.95,           // Nucleus sampling
  do_sample: true,       // Enable sampling
});
```

### UI Styling:
The CSS is embedded in `index.html` for simplicity. Key classes:
- `.container` - Main chat container
- `.message` - Chat message styling
- `.user-message` / `.ai-message` - Message types
- `.loading-overlay` - Model loading screen

## ğŸš¨ Troubleshooting

### Common Issues:

**1. Automatic Fallback Working:**
```
Info: WebGPU not available, using WASM
Solution: No action needed - app will work with WASM
```

**2. Model Loading Fails:**
```
Error: Failed to load model
Solution: Check network connection, clear browser cache
```

**3. Out of Memory:**
```
Error: GPU memory exceeded
Solution: Close other tabs, reduce max_new_tokens
```

**4. Slow Performance:**
```
Issue: Responses take too long
Solution: Use WebGPU-enabled browser, close other tabs, or use WASM mode
```

## ğŸ¤ Contributing

1. Fork the repository
2. Create feature branch (`git checkout -b feature/amazing-feature`)
3. Commit changes (`git commit -m 'Add amazing feature'`)
4. Push to branch (`git push origin feature/amazing-feature`)
5. Open Pull Request

## ğŸ“„ License

MIT License - feel free to use this project for learning and development!

## ğŸ™ Acknowledgments

### Core Technologies
- **HuggingFace Transformers.js** - Browser-based ML inference framework ([Docs](https://huggingface.co/docs/transformers.js/en/index))
- **Google Gemma 3 270M ONNX** - Open language model optimized for browsers ([Model](https://huggingface.co/onnx-community/gemma-3-270m-it-ONNX), [Base](https://huggingface.co/google/gemma-3-270m))
- **WebGPU** - GPU acceleration in browsers
- **WebAssembly (WASM)** - High-performance browser runtime
- **Vite** - Fast development and build tool

### Research & Inspiration
- **Simon Willison** - Extensive documentation of browser AI capabilities and Transformers.js applications, including early testing of Gemma 3 270M as "absolutely tiny, just a 241MB download" ([Tweet](https://x.com/simonw/status/1956044677796577457))
- **Google DeepMind** - Gemma model family and smaller efficient models approach ([Gemma 3](https://blog.google/technology/developers/gemma-3/), [Gemma 3 270M](https://developers.googleblog.com/en/introducing-gemma-3-270m/))
- **WebLLM Project** - High-performance in-browser LLM inference engine research
- **Chrome Developer Relations** - WebGPU and WebAssembly AI performance enhancements

### Key Academic References
- Ruan, C.F., Qin, Y., Zhou, X., et al. (2024). "WebLLM: A High-Performance In-Browser LLM Inference Engine." arXiv:2412.15803
- Google DeepMind (2024). "Gemma 3: Google's new open model based on Gemini 2.0"
- Chrome for Developers (2024). "WebAssembly and WebGPU enhancements for faster Web AI"

### Community Inspiration  
- **Browser AI Community** - Pioneering local inference and privacy-first approaches
- **Open Source AI Movement** - Making advanced language models accessible to everyone
- **Educational AI Projects** - Including bedtime story generators ([svharivinod/ai-story-generator](https://github.com/svharivinod/ai-story-generator)) and interactive learning tools
- **Fully Local PDF Chatbot** - Complete RAG pipeline inspiration ([jacoblee93/fully-local-pdf-chatbot](https://github.com/jacoblee93/fully-local-pdf-chatbot))

## ğŸ“ˆ Project Status

**Current Status**: âœ… **FULLY OPERATIONAL** (Aug 29, 2025)

### Recent Updates:
- âœ… **Universal Browser Support** - Automatic WebGPU/WASM fallback system
- âœ… **Network Accessibility** - Auto-detects local IP for cross-device access
- âœ… **Comprehensive Testing Suite** - Playwright tests with multiple browser support
- âœ… **Performance Optimized** - Real-time streaming, Web Workers, memory efficient
- âœ… **Production Ready** - Complete error handling, fallbacks, and user experience
- âœ… **NPX Ready** - One-command GitHub execution via `npx github:kaiser/gemma-chat-app`

### Verification Results:
- **Model Loading**: âœ… Gemma 270M ONNX downloads and initializes correctly
- **Device Detection**: âœ… Automatic WebGPU/WASM selection and fallback
- **Network Access**: âœ… Auto-detects local IP for cross-device compatibility
- **Chat Interface**: âœ… Real-time streaming responses with typing indicators
- **Context Management**: âœ… Multi-turn conversations with history
- **Error Recovery**: âœ… Graceful handling of device failures and network issues
- **Memory Management**: âœ… Efficient model loading and cleanup

### Performance Benchmarks:
```
âœ… Model Download: ~270MB (one-time)
âœ… Initialization: 30-120s first run, 5-15s cached
âœ… Response Time: 2-10 seconds with real-time streaming
âœ… Memory Usage: 400-800MB during active chat
âœ… Token Generation: 10-50 tokens/sec (WebGPU), 5-25 tokens/sec (WASM)
âœ… Browser Support: Universal - Chrome, Firefox, Safari, Edge
```

### Development Environment:
- **Server**: http://localhost:3001 + network IP (auto-detected)
- **Testing**: Comprehensive Playwright multi-browser test suite
- **Build**: Production-ready Vite build system
- **Dependencies**: HuggingFace Transformers.js 3.0.2, WebGPU/WASM support

---

**Built with â¤ï¸ for the browser AI community**

*Ready for production deployment and educational use*